## í•µì‹¬ í‚¤ì›Œë“œ
> **ì •í˜• ë°ì´í„°(structured data)**: êµ¬ì¡°ë¥¼ ê°–ì¶˜ ë°ì´í„°ë¥¼ ì´ë¥´ëŠ” ë§. í–‰ê³¼ ì—´ë¡œ ê° ìƒ˜í”Œê³¼ íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„°ë¥¼ ì •í˜• ë°ì´í„°ì˜ ì˜ˆì‹œë¡œ ë“¤ ìˆ˜ ìˆë‹¤.

> **ë¹„ì •í˜• ë°ì´í„°(unstructured data)**: íŠ¹ì •í•œ êµ¬ì¡°ê°€ ì—†ì–´, ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ì—‘ì…€ë¡œ í‘œí˜„í•˜ê¸° ì–´ë µë‹¤. í…ìŠ¤íŠ¸ ë°ì´í„°, ë””ì§€í„¸ ì‚¬ì§„, ë””ì§€í„¸ ìŒì•… ë“±ì´ ì˜ˆì‹œì´ë‹¤.

> **ì•™ìƒë¸” í•™ìŠµ(ensemble learning)**: ì—¬ëŸ¬ ê°œì˜ ê°œë³„ ëª¨ë¸(ì•½í•œ í•™ìŠµê¸°, base learner)ì„ ê²°í•©í•˜ì—¬ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ë” ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì–»ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•.
> ì´ëŸ¬í•œ ë°©ì‹ì€ ë¶„ì‚°ì„ ê°ì†Œì‹œì¼œ ê³¼ëŒ€ì í•©ì— ê°•í•˜ê³ , í¸í–¥ì„ ê°ì†Œì‹œì¼œ ë‹¨ìˆœ ëª¨ë¸ì˜ í•œê³„ë¥¼ ë³´ì™„í•œë‹¤. ë˜í•œ ë°ì´í„° ë³€ë™ì— ëœ ë¯¼ê°í•˜ì—¬ ì•ˆì •ì„±ì´ ë†’ë‹¤.

> **ëœë¤ í¬ë ˆìŠ¤íŠ¸(random forest)**: ì•™ìƒë¸” í•™ìŠµì˜ ëŒ€í‘œì ì¸ ëª¨ë¸ ì¤‘ í•˜ë‚˜. ìƒ˜í”Œ ë°ì´í„° ì¤‘ì—ì„œ ì¤‘ë³µì„ í—ˆìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë½‘ì€ ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œì„ ì´ìš©í•´ ì—¬ëŸ¬ ê°œì˜ ê²°ì • íŠ¸ë¦¬ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤.
> ê²°ì • íŠ¸ë¦¬ì˜ ê° ë…¸ë“œë¥¼ ë¶„í• í•  ë•Œ ì¼ë¶€ íŠ¹ì„±ì„ ë¬´ì‘ìœ„ë¡œ ê³ ë¥¸ ë‹¤ìŒ, ìµœì„ ì˜ ë¶„í• (ì •ë³´ ì´ë“ ìµœëŒ€)ì„ ì°¾ëŠ”ë‹¤. RandomForestClassifierì˜ ê²½ìš°, ê¸°ë³¸ì ìœ¼ë¡œ ì „ì²´ íŠ¹ì„± ê°œìˆ˜ì˜ ì œê³±ê·¼ë§Œí¼ì˜ íŠ¹ì„±ì„ ì„ íƒí•œë‹¤.
> ì´ëŸ° ë°©ì‹ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ í›ˆë ¨í•œ í›„, ë¶„ë¥˜ì˜ ê²½ìš° ê° íŠ¸ë¦¬ì˜ í´ë˜ìŠ¤ë³„ í™•ë¥ ì„ í‰ê· í•´ ê°€ì¥ ë†’ì€ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡ìœ¼ë¡œ ì‚¼ëŠ”ë‹¤. íšŒê·€ì¼ ë•ŒëŠ” ë‹¨ìˆœíˆ ê° íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ì„ í‰ê· í•œë‹¤.

> **ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬(extra tree)**: ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ ìœ ì‚¬í•œ ì•™ìƒë¸” ì•Œê³ ë¦¬ì¦˜. ê²°ì • ë°ì´í„°ë¥¼ ë§Œë“¤ ë•Œ ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•œë‹¤.
> ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬ì˜ ê° ê²°ì • íŠ¸ë¦¬ëŠ” ë…¸ë“œ ë¶„í•  ì‹œ ì¼ë¶€ íŠ¹ì„±ì„ ëœë¤ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ë¶„í• í•˜ì§€ë§Œ, ìµœì„ ì˜ ë¶„í• ì„ ì°¾ì§€ ì•Šê³  ë¬´ì‘ìœ„ë¡œ ë¶„í• í•œë‹¤.
> ì¦‰, ì—‘ìŠ¤íŠ¸ë¼ íŠ¸ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ê²°ì • íŠ¸ë¦¬ëŠ” DecisionTreeClassifierì˜ *splitter* ë§¤ê°œë³€ìˆ˜ ê°’ì„ 'random'ìœ¼ë¡œ ì¤€ ê²°ì • íŠ¸ë¦¬ì´ë‹¤.

> **ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…(gradient boosting)**  
> : ê¹Šì´ê°€ ì–•ì€ ê²°ì • íŠ¸ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€í•˜ì—¬ ì´ì „ ëª¨ë¸ì˜ ì˜¤ì°¨ë¥¼ ë³´ì •í•˜ëŠ” ì•™ìƒë¸” í•™ìŠµ ë°©ë²•ì´ë‹¤. ì „ì²´ ëª¨ë¸ì€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ê²½ì‚¬ í•˜ê°• ë°©ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸ëœë‹¤.  
> ì´ˆê¸°ì—ëŠ” ìƒìˆ˜ í•¨ìˆ˜ ğ¹0ë¥¼ ì„¤ì •í•˜ê³ , ì´í›„ ê° ë‹¨ê³„ì—ì„œëŠ” í˜„ì¬ ì˜ˆì¸¡ê°’ì—ì„œ ê³„ì‚°ëœ ì†ì‹¤í•¨ìˆ˜ì˜ ìŒì˜ gradient ê°’(pseudo-residual)ì„ ì…ë ¥ íŠ¹ì„± ê³µê°„ì—ì„œ í•¨ìˆ˜ë¡œ ê·¼ì‚¬í•˜ëŠ” íšŒê·€ íŠ¸ë¦¬ë¥¼ í•™ìŠµí•œë‹¤.  
> ì´ëŠ” ëª¨ë¸ì´ í˜„ì¬ ì˜ˆì¸¡ì—ì„œ í‹€ë¦° ë°©í–¥ê³¼ í¬ê¸°ë¥¼ í•™ìŠµí•˜ëŠ” ê³¼ì •ì´ë‹¤.  
> ê° íŠ¸ë¦¬ëŠ” ìƒ˜í”Œì˜ íŠ¹ì„± ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê¹Šì´ê°€ ë§¤ìš° ì–•ì€ íšŒê·€ íŠ¸ë¦¬ì´ë©°, ë¶„í•  ì‹œ pseudo-residualì— ëŒ€í•œ  
> Î” = SSEparent âˆ’ (SSEleft + SSEright) ê°€ ìµœëŒ€ê°€ ë˜ë„ë¡ ë¶„í• í•œë‹¤.  
> Î”ê°€ ìµœëŒ€ê°€ ë˜ë©´ ê° ë¦¬í”„ ë…¸ë“œ ë‚´ì˜ pseudo-residual ë¶„ì‚°ì´ ê°ì†Œí•˜ë©°, ì´ëŠ” ë™ì¼í•œ ìƒìˆ˜ ë³´ì •ê°’ì„ ì ìš©í•´ë„ í•´ë‹¹ ë…¸ë“œì— ì†í•œ ìƒ˜í”Œë“¤ì˜ ì†ì‹¤ì´ íš¨ê³¼ì ìœ¼ë¡œ ê°ì†Œí•¨ì„ ì˜ë¯¸í•œë‹¤.  
> ì´ ê³¼ì •ì€ ì‹¤ì œ í´ë˜ìŠ¤ì™€ ë¬´ê´€í•˜ê²Œ ì´ë£¨ì–´ì§€ë©°, ë¦¬í”„ ë…¸ë“œì˜ ê°’ì€ í•´ë‹¹ ë…¸ë“œ ë‚´ ìƒ˜í”Œë“¤ì˜ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë„ë¡ ê³„ì‚°ëœë‹¤.  
> ì „ì²´ ëª¨ë¸ì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
> <img width="264" height="66" alt="image" src="https://github.com/user-attachments/assets/e0d4ad1b-56eb-42f7-b5b2-d2abe4d57c08" />  
> F0(x): ì´ˆê¸° ìƒìˆ˜ 
> Î·: learning rate 
> ê° â„ğ‘š(ğ‘¥): ì…ë ¥ ì˜ì¡´ì  ë³´ì • í•¨ìˆ˜  
> ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì˜ ì†ë„ë¥¼ ê°œì„ í•œ ë²„ì „ì¸ íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì´ ì•ˆì •ì ì¸ ê²°ê³¼ì™€ ë†’ì€ ì„±ëŠ¥ìœ¼ë¡œ ì¸ê¸°ê°€ ë†’ë‹¤.


## ì½”ë“œ ì „ë¬¸
```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

wine = pd.read_csv('https://bit.ly/wine_csv_data')

data = wine[['alcohol', 'sugar', 'pH']]
target = wine['class']

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)

### Randomized Forest
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_jobs=-1, random_state=42)
"""
cross_validate() í•¨ìˆ˜ëŠ” dictionaryë¥¼ ë°˜í™˜í•œë‹¤.
keyëŠ” 'fit_time', 'score_time', 'test_score', return_train_scoreë¥¼ Trueë¡œ ì§€ì • ì‹œ 'train_score', 'train_accuracy', 'train_f1'ì´ ì¶”ê°€ëœë‹¤.
"""
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
rf.fit(train_input, train_target)
print(rf.feature_importances_)
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)

rf.fit(train_input, train_target)
print(rf.oob_score_)

### Extra Tree
from sklearn.ensemble import ExtraTreesClassifier

et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target,
                        return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

et.fit(train_input, train_target)
print(et.feature_importances_)

### gradient boosting
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target,
                        return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2,
                                random_state=42)
scores = cross_validate(gb, train_input, train_target,
                        return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

gb.fit(train_input, train_target)
print(gb.feature_importances_)

### hist gradient boosting
from sklearn.ensemble import HistGradientBoostingClassifier

hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target,
                        return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
from sklearn.inspection import permutation_importance

hgb.fit(train_input, train_target)
result = permutation_importance(hgb, train_input, train_target, n_repeats=10,
                                random_state=42, n_jobs=-1)
print(result.importances_mean)
[0.08876275 0.23438522 0.08027708]
result = permutation_importance(hgb, test_input, test_target, n_repeats=10,
                                random_state=42, n_jobs=-1)
print(result.importances_mean)
hgb.score(test_input, test_target)

# XGBoost
from xgboost import XGBClassifier

xgb = XGBClassifier(tree_method='hist', random_state=42)
xgb._estimator_type = "classifier"
scores = cross_validate(xgb, train_input, train_target,
                        return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
LightGBM

### LightGBM
from lightgbm import LGBMClassifier

lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target,
                        return_train_score=True, n_jobs=-1)

print(np.mean(scores['train_score']), np.mean(scores['test_score']))

```
